[["index.html", "Couscous: A Causal Inference Cookbook in R Preface", " Couscous: A Causal Inference Cookbook in R Theo Obadiah Teguh 2024-03-24 Preface After a few years in my undergraduate economics program, I grew interest in the field of programming. I felt that in the real world, the concepts and theories in the field of economics and finance are brought alive with data. Times are changing, and the ability to code and create elegant computer scripts will be very useful, especially when handling data sets and conducting analysis. I realized that unless you were on your way towards a computer or data science degree, there wasn’t really a concrete way of getting used to working with data and programming on your own free time. Additionally, I wanted the content of this book to assist my fellow students in my program. Data: the way you see it in movies Therefore, I compiled this book. The purpose of this book is to storehouse all the foundations you need as a fresh beginner in statistical programming. Whether you are an undergraduate or postgraduate, this book will help you get started with R language programming and data analysis. This book will also dive deeper into the methodologies of causal inference, which will be particularly useful for final year research projects, postgraduate statistical studies, and research. The content of this book is based on certain lectures and coursework that I have received throughout university, my personal study notes, as well as publicly available sources that I have encountered and used in my journey. I believe anyone can code with R, but the question is how and where should one start? Couscous (a pun of ‘cause-cause’) aims to be the solution for you. By the end of reading this book, you will be able to use the R language effectively for causal inference. "],["usage-of-this-book.html", "Usage of this book", " Usage of this book This book is meant to be used as a supplement to your coursework in university. It covers the basics of the R programming language and important libraries such as tidyverse, knitr, and stargazer. It will also cover certain statistical topics such as econometric foundations, Randomized Controlled Trials (RCT), Instrumental Variables (IV), Regression Discontinuity Design (RDD), and Difference-in-Differences (DiD). It is recommended to go through the chapters of this book in order, as early chapters serve as a foundation for the later sections. There will also be recommended and supplementary materials sections throughout the book. The recommended materials section is provided with hopes of strengthening certain concepts. They will be very significant and helpful for your understanding. The supplementary materials section aims to provide you with additional context on topics that you are interested in. Related exercises will be provided in sections of the book. They are sourced from other textbooks and problem sets available online. "],["software-information.html", "Software Information", " Software Information This is an overview of the different types of software we use in this book. R is a free software environment for statistical computing and graphics, that can be downloaded online. R Studio is a well-known Integrated Development Environment (IDE) for R. If you don’t know what that means, it’s just basically where you type in your code and see the targeted outputs. This software is also available for free online. R Markdown is a powerful tool that can be used alongside R Studio to create beautiful documents and format them neatly in PDF, HTML, or other formats. Though not compulsory, it is highly recommended for you to learn about this software as it is quite easy to pickup once you understand R syntax, and the results can be spectacular. You can also check out the documentation online. Finally, this book was created with R Bookdown, which compiles your R Markdown files into production-ready booklets. It is completely free and the documentation is available online. Knowledge of this is not required. "],["acknowledgments.html", "Acknowledgments", " Acknowledgments This book was mainly inspired by Dr. Yanhui Wu’s course: ECON3284 Causal Inference. I took this course in the 2023 Fall Semester with the goal of overcoming my fear of statistics. It was a terrific experience with stellar teaching, and I got to meet a lot of new like-minded friends and colleagues. Dr. Yanhui Wu, University of Hong Kong Additionally, the book utilizes a lot of ideas and concepts from the Introduction to Data Science and Advanced Data Science textbooks by HarvardX. These books are available for free online, and were used in the course: COMP2501 Introduction to Data Science and Engineering at the University of Hong Kong. Introduction to Data Science, HarvardX Finally, the videos embedded throughout this book are sourced from the StatQuest Youtube Channel by Josh Starmer. His videos have been a lifesaver for myself and a lot of my friends. Josh Starmer, StatQuest "],["about-the-author.html", "About the Author", " About the Author Theo Obadiah Teguh is a student in the Bachelor of Economics and Finance program by the University of Hong Kong (HKU). He is interested in quantitative research and statistical analysis, and is taking computer science as a second major. He worked as a data research assistant at the Faculty of Business and Economics at HKU under the supervision of Dr. Yang Liu alongside other PhD students, and has assisted in projects related to XML scraping, machine learning, and data engineering. In the fall semester of 2023, he worked as a Student Teaching Assistant for the Department of Computer Science, and studied causal inference under the guidance of Dr. Yanhui Wu. He loves spicy Indian and Sichuan food, as much as basketball. You can find him on GitHub and LinkedIn. "],["rfoundations.html", "1 R Foundations", " 1 R Foundations Basic Syntax Visualization with R {#rfoundations} Visualization with ggplot2 {#rfoundations} "],["sfoundations.html", "2 Statistical Foundations", " 2 Statistical Foundations P-Values {#sfoundations} Hypothesis Testing {#sfoundations} "],["regression.html", "3 Statistics and Regression with R 3.1 Linear Regression 3.2 Multiple Linear Regression 3.3 Adding Interaction Variables 3.4 Putting it All Together", " 3 Statistics and Regression with R Before we start doing any analysis, let’s setup our working directory. Moreover, because the data sets we’ll be working with are relatively small, we can just load both files and assign them to two unique variables. # Here we setup our working directory, i.e. the location of the folder we are using setwd(&quot;/Users/theoobadiahteguh/R_Programming/ECON3284/ProblemSet1&quot;) # Now we combine the directory path and the file name to obtain the full path advertising &lt;- read.csv(file.path(getwd(), &quot;Advertising.csv&quot;)) auto &lt;- read.csv(file.path(getwd(), &quot;Auto.csv&quot;)) Now that we have loaded the data, we can begin our analysis and answer the questions in the second section. 3.1 Linear Regression For questions 1 to 3 we will use the “Advertising” data set. Let’s first see the structure of the data: str(auto) ## &#39;data.frame&#39;: 392 obs. of 9 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cylinders : int 8 8 8 8 8 8 8 8 8 8 ... ## $ displacement: num 307 350 318 304 302 429 454 440 455 390 ... ## $ horsepower : int 130 165 150 150 140 198 220 215 225 190 ... ## $ weight : int 3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ... ## $ acceleration: num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ origin : int 1 1 1 1 1 1 1 1 1 1 ... ## $ name : chr &quot;chevrolet chevelle malibu&quot; &quot;buick skylark 320&quot; &quot;plymouth satellite&quot; &quot;amc rebel sst&quot; ... Let us now create the model in our R environment and create a plot to visualize the data with the model: model_q1 &lt;- lm(sales ~ TV, advertising) attach(advertising) plot(TV, sales, xlab = &quot;TV Advertising Budget&quot;, ylab = &quot;Sales&quot;) abline(model_q1, col = &quot;navy&quot;) We can also generate a summary of the model: summary(model_q1) ## ## Call: ## lm(formula = sales ~ TV, data = advertising) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.3860 -1.9545 -0.1913 2.0671 7.2124 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.032594 0.457843 15.36 &lt;2e-16 *** ## TV 0.047537 0.002691 17.67 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.259 on 198 degrees of freedom ## Multiple R-squared: 0.6119, Adjusted R-squared: 0.6099 ## F-statistic: 312.1 on 1 and 198 DF, p-value: &lt; 2.2e-16 From this summary we can answer parts (a) to (c): a.) Beta(1) = 0.047537 b.) Standard Error of Beta(1) = 0.002691 c.) The t-statistic of Beta(1) = 17.67 and it is statistically significant as it has a p-value smaller than 0.001, and thus we reject the null hypothesis. d.) If there is zero budget for TV Advertising, then the sales will simply be the y-intercept of the model, that is equal to 7.032594. e.) Finally the R-Squared is 0.6119 and we can interpret it as “61.19% of the change in sales can be explained by the variability of TV Advertising.” 3.2 Multiple Linear Regression Let us now create the second model. We will not visualize this model but we can still generate a cohesive summary. model_q2 &lt;- lm(sales ~ TV + radio + newspaper, advertising) summary(model_q2) ## ## Call: ## lm(formula = sales ~ TV + radio + newspaper, data = advertising) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.8277 -0.8908 0.2418 1.1893 2.8292 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.938889 0.311908 9.422 &lt;2e-16 *** ## TV 0.045765 0.001395 32.809 &lt;2e-16 *** ## radio 0.188530 0.008611 21.893 &lt;2e-16 *** ## newspaper -0.001037 0.005871 -0.177 0.86 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.686 on 196 degrees of freedom ## Multiple R-squared: 0.8972, Adjusted R-squared: 0.8956 ## F-statistic: 570.3 on 3 and 196 DF, p-value: &lt; 2.2e-16 Thus, we can answer the questions based on the summary: a.) Estimated beta(2) = 0.188530 b.) Standard error of beta(3): = 0.005871 c.) The t-statistic of beta(3) = -0.177 and the p-value is much larger than 0.1, so we fail to reject the null hypothesis and it is not statistically significant. d.) Adjusted R-Squared = 0.8956 and it can be explained as “the variability of Y that can be explained by the regression model, i.e. the variability of X1, X2, and X3”. In other words, 89.56% of the variability of Sales can be explained by the TV, Radio, and Newspaper variables. e.) The F-statistic = 570.3 and as the p-value of F-statistic value is nearly 0 which is much smaller than 0.001, we can conclude that at least one of the coefficients is non-zero. 3.3 Adding Interaction Variables Let us now create the third model. Again, we will not visualize this model but we can still generate a cohesive summary. model_q3 &lt;- lm(sales ~ TV + radio + TV : radio, advertising) summary(model_q3) ## ## Call: ## lm(formula = sales ~ TV + radio + TV:radio, data = advertising) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.3366 -0.4028 0.1831 0.5948 1.5246 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.750e+00 2.479e-01 27.233 &lt;2e-16 *** ## TV 1.910e-02 1.504e-03 12.699 &lt;2e-16 *** ## radio 2.886e-02 8.905e-03 3.241 0.0014 ** ## TV:radio 1.086e-03 5.242e-05 20.727 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9435 on 196 degrees of freedom ## Multiple R-squared: 0.9678, Adjusted R-squared: 0.9673 ## F-statistic: 1963 on 3 and 196 DF, p-value: &lt; 2.2e-16 From this summary we can answer part (a), and find the value of beta(3) in the following manner so that it is readable: summary(model_q3)$coefficients[4] ## [1] 0.001086495 And then, we can calculate part (b) as follows: 5 * summary(model_q3)$coefficients[3] + summary(model_q3)$coefficients[4] * 200 * 5 ## [1] 1.230796 3.4 Putting it All Together Now, for this question we will use the “Auto” data set. Let’s first see the structure of the data: str(auto) ## &#39;data.frame&#39;: 392 obs. of 9 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cylinders : int 8 8 8 8 8 8 8 8 8 8 ... ## $ displacement: num 307 350 318 304 302 429 454 440 455 390 ... ## $ horsepower : int 130 165 150 150 140 198 220 215 225 190 ... ## $ weight : int 3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ... ## $ acceleration: num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ origin : int 1 1 1 1 1 1 1 1 1 1 ... ## $ name : chr &quot;chevrolet chevelle malibu&quot; &quot;buick skylark 320&quot; &quot;plymouth satellite&quot; &quot;amc rebel sst&quot; ... a.) We will now create a linear model with mpg as the response and horsepower as the predictor, and generate a summary # Creating the model model_q4a &lt;- lm(mpg ~ horsepower, auto) # Generating the summary summary(model_q4a) ## ## Call: ## lm(formula = mpg ~ horsepower, data = auto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.5710 -3.2592 -0.3435 2.7630 16.9240 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.935861 0.717499 55.66 &lt;2e-16 *** ## horsepower -0.157845 0.006446 -24.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.906 on 390 degrees of freedom ## Multiple R-squared: 0.6059, Adjusted R-squared: 0.6049 ## F-statistic: 599.7 on 1 and 390 DF, p-value: &lt; 2.2e-16 From the summary, we can clearly see that: i.) There is a relationship between our predictor and response, and our predictor is statistically significant as the p-value is less than 0.001. ii.) We see that the value of R-Squared is 0.6059, and the relationship is somewhat at moderate strength. iii.) The relationship is a negative relationship, as reflected by beta(horsepower) of -0.157845. iv.) If we have 98 horsepower then we would predict the mpg to be: summary(model_q4a)$ coefficients[1] + 98 * summary(model_q4a)$coefficients[2] ## [1] 24.46708 Additionally, we can also calculate the 95% confidence intervals: # We can use this for the confidence interval of the model confint(model_q4a, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 38.525212 41.3465103 ## horsepower -0.170517 -0.1451725 # We can use this for the confidence interval of the prediction if we have 98 horsepower # We can input the values of one or more predictors in the data frame predict(model_q4a, data.frame(horsepower = 98), interval = &quot;confidence&quot;) ## fit lwr upr ## 1 24.46708 23.97308 24.96108 b.) We can also plot our model as follows: attach(auto) plot(horsepower, mpg, xlab = &quot;Horsepower&quot;, ylab = &quot;Miles per Gallon&quot;) abline(model_q4a, col = &quot;navy&quot;) c.) We will now create diagnostic plots. To do this, we use the broom library: library(broom) model.diag.metrics &lt;- augment(model_q4a) par(mfrow = c(2, 2)) plot(model_q4a) There are a few things that have been highlighted by the diagnostic plots: 1.) The Residuals vs Fitted plot has a slightly curved convex line, suggesting that the relationship may not be linear. 2.) The Normal Q-Q plot shows that the residual points follow the straight dashed line relatively well, although it is also slightly convex in shape. This suggests that the residuals may not completely follow a normal distribution. 3.) The Scale-Location (or Spread-Location) plot shows a convex or bent v-shaped line with a high concentration of points on the right, suggesting that we may have a heteroscedasticity problem. 4.) The Residuals vs Leverage plot shows that there aren’t any “influential” variables. d.) We will now create a scatter plot matrix with base R: plot(auto) Alternatively, a more reasonable and tidier way is to use the function pairs(), and we must exclude the “name” variable as it is qualitative: # We can set the plotting &quot;character&quot; with the pch value pairs(auto[,-9], pch = 18) e.) We can also create a matrix of correlations and visualize it with the corrplot library: # We exclude the &quot;name&quot; column from the data frame corr_matrix &lt;- cor(auto[, -9]) corr_matrix ## mpg cylinders displacement horsepower weight ## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 ## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 ## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 ## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 ## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 ## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 ## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 ## origin 0.5652088 -0.5689316 -0.6145351 -0.4551715 -0.5850054 ## acceleration year origin ## mpg 0.4233285 0.5805410 0.5652088 ## cylinders -0.5046834 -0.3456474 -0.5689316 ## displacement -0.5438005 -0.3698552 -0.6145351 ## horsepower -0.6891955 -0.4163615 -0.4551715 ## weight -0.4168392 -0.3091199 -0.5850054 ## acceleration 1.0000000 0.2903161 0.2127458 ## year 0.2903161 1.0000000 0.1815277 ## origin 0.2127458 0.1815277 1.0000000 library(corrplot) ## corrplot 0.92 loaded # Generating the matrix visualization corrplot(corr_matrix, method = &quot;color&quot;) f.) Finally we can create a multiple linear regression model from the Auto data set and generate its summary: # Here we include every variable except for &quot;name&quot; model_q4f &lt;- lm(mpg ~ .-name, auto) summary(model_q4f) ## ## Call: ## lm(formula = mpg ~ . - name, data = auto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.5903 -2.1565 -0.1169 1.8690 13.0604 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.218435 4.644294 -3.707 0.00024 *** ## cylinders -0.493376 0.323282 -1.526 0.12780 ## displacement 0.019896 0.007515 2.647 0.00844 ** ## horsepower -0.016951 0.013787 -1.230 0.21963 ## weight -0.006474 0.000652 -9.929 &lt; 2e-16 *** ## acceleration 0.080576 0.098845 0.815 0.41548 ## year 0.750773 0.050973 14.729 &lt; 2e-16 *** ## origin 1.426141 0.278136 5.127 4.67e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.328 on 384 degrees of freedom ## Multiple R-squared: 0.8215, Adjusted R-squared: 0.8182 ## F-statistic: 252.4 on 7 and 384 DF, p-value: &lt; 2.2e-16 We can now answer the following sections: i.) There is a relationship between the predictors and the response. The model has an F-statistic of 252.4 with a p-value of less than 0.001 classifying it as statistically significant. Moreover, the relationship of the predictors and the response is quite strong as the adjusted R-Squared value is 0.8182. ii.) The “displacement” predictor is statistically significant with a p-value less than 0.01. We also have weight, year, and origin with p-values smaller than 0.001. iii.) The year coefficient suggests that over time, the number of miles per gallon increases. This makes sense because as time passes we expect technological improvement such that engines become more efficient. g.) We then plot the diagnostic plots as we did in a previous section: # library(broom) model.diag.metrics &lt;- augment(model_q4f) par(mfrow = c(2, 2)) plot(model_q4f) There are a few things that have been highlighted by the diagnostic plots: 1.) The Residuals vs Fitted plot has a slightly curved convex line, suggesting that the relationship may not be linear. 2.) The Normal Q-Q plot shows that the residual points somewhat follow the straight dashed line, although it deviates upward on the right side of the plot. This suggests that the residuals may not completely follow a normal distribution. 3.) The Scale-Location (or Spread-Location) plot shows a no perfectly straight horizontal line with a high concentration of points on the middle and the right, suggesting that we may have a heteroscedasticity problem. 4.) The Residuals vs Leverage plot shows that there aren’t any “influential” variables, as no observation falls outside the Cook’s distance line. Thus, we conclude this first problem set. "],["rct.html", "4 Randomized Controlled Trials 4.1 Methodology 4.2 Neyman-Rubin Model 4.3 Selection Bias 4.4 Counterfactual Framework 4.5 Framework and Assumptions 4.6 Caveats and Difficulties 4.7 Case Studies", " 4 Randomized Controlled Trials 4.1 Methodology In this chapter, we will go over the basic intuition that is required for carrying out Randomized Control Trials (RCT) and the fundamental problem of causal inference. We will also discuss several potential solutions and caveats. 4.2 Neyman-Rubin Model How do we determine the causal effect of some treatment? Intuitively, we can just compare the outcome between two groups: treatment (treated) and control (untreated). Neyman-Rubin defined the causal effect \\(\\delta\\) as follows: \\[\\begin{equation} \\delta=Y^{Treatment}-Y^{Control} \\tag{4.1} \\end{equation}\\] However, now we encounter a major issue. How do we know if this comparison is fair? How do we know that we are comparing apples with apples and not apples with oranges? 4.3 Selection Bias Imagine that you are comparing the effect of health insurance on peoples health. You gather a pool of people and separate them in two groups. People with health insurance are classified as the treatment, and people with no health insurance are placed into the control group. Suppose that you create a sophisticated regression model and you find that people with health insurance tend to have better health compared to the people without health insurance. Does this mean that health insurance causes healthier lifestyle? We can actually argue that people who have access to health insurance may have higher levels of income. Thus, this group of people may have access to better food, better living conditions, access to a gym, and so on which may all lead to better health. Therefore, this comparison is not fair, we cannot simply subtract the difference in outcomes, because we are comparing ‘apples with oranges’. This phenomenon is something that we refer to as selection bias. We define it as the bias driven by the systematic differences between the treatment (e.g insured) and control (e.g uninsured) groups. The solution to this is ceteris paribus or ‘all other things equal’, the favorite Latin assumption for economists. However, in the field social studies, this is fundamentally difficult to attain as all individuals are inherently different. Potential remedies to this selection effect include: Adding: Add more control variables to mitigate the omitted variable bias, a very popular and cheap, but it is not really a solution. Matching: Choose a comparison group that is close to the group of interest, but we will still encounter problems with unobserved variables. Modeling: Explicitly model the selection process (e.g Heckman selection), with the risk of modeling human behavior wrongly. Experiments: Randomized Controlled Trials (RCT). 4.4 Counterfactual Framework So how do we deal with apples and oranges? The best comparison (control) group for an interest (treatment) group is a group that only differs in treatment but not any other features. So instead we ask the question: What would be the potential outcome if the treatment did not happen? We will start exploring this question by understanding some crucial concepts. Note that we want to focus more on average treatment effect instead of individual treatment effect because we are studying general human behavior and statistically, we need to calculate the average of a sufficient large group of people to remove the effect driven by chance or noise. 4.4.1 Defining ATE The Average Treatment Effect (ATE) is defined as follows: \\[\\begin{equation} \\begin{split} \\mathrm{ATE} &amp; =\\text{mean}(Y^{\\text{Treated}})-\\text{mean}(Y^{\\text{Control}})\\\\ &amp; =\\text{mean}(Y^{\\text{Treated}}-Y^{\\text{Control}}) \\end{split} \\tag{4.2} \\end{equation}\\] Visualizing ATE with R Let’s try to illustrate and visualize ATE with R. First, we generate hypothetical control and treatment groups as follows: # We randomly generate 100 data points for both groups # The control follows a normal distribution with mean = 5 and std = 2 y_control &lt;- rnorm(100, mean = 5, sd = 2) # The treatment follows a normal distribution with mean = 10 and std = 3 y_treatment &lt;- rnorm(100, mean = 10, sd = 3) We find ATE by calculating the difference in their means: mean(y_treatment) - mean(y_control) ## [1] 5.277196 We can visualize the ATE by plotting the density curves for the control and treatment groups and mark out the means with dashed lines. library(ggplot2) ## ## Attaching package: &#39;ggplot2&#39; ## The following object is masked from &#39;auto&#39;: ## ## mpg library(ggthemes) library(plyr) # We combine the data into one larger data frame and create categories ate_df &lt;- data.frame(Control = y_control, Treatment = y_treatment) # Now, we stack the two columns into one ate_df &lt;- stack(lapply(ate_df[1:2], as.numeric)) names(ate_df) &lt;- c(&quot;Values&quot;, &quot;Group&quot;) After the data transformation above, our data will look something like this. Table 4.1: A glimpse of ate_df Values Group 98 4.814835 Control 99 2.202815 Control 100 3.760091 Control 101 14.725563 Treatment 102 13.203763 Treatment 103 11.767418 Treatment We can continue plotting the data. In this plot, ATE is just the distance between the two dashed lines. # Now we create a summary table for the means mu &lt;- ddply(ate_df, &quot;Group&quot;, summarise, grp.mean = mean(Values)) ate_df |&gt; ggplot(aes(x = Values, color = Group)) + # Plot the density curves geom_density(linewidth = 0.8, alpha = 0.8, bw = 0.6679) + # Add the dashed lines marking the averages geom_vline(data = mu, aes(xintercept = grp.mean, color = Group), linetype = &quot;dashed&quot;) + # Add some additional details theme_clean() + ylab(&quot;Density&quot;) + labs(caption = &quot;N = 100 Bandwith = 0.6679&quot;) + scale_color_manual(values=c(&quot;black&quot;, &quot;red&quot;)) 4.4.2 Defining ATT The Average Treatment Effect on the Treated (ATT) is defined as follows: \\[\\begin{equation} \\begin{split} \\mathrm{ATT} &amp; =\\text{mean}(Y^{\\text{Treated|Treated}})-\\text{mean}(Y^{\\text{Control|Treated}})\\\\ &amp; =\\text{mean}(Y^{\\text{Treated}}-Y^{\\text{Control}}|\\text{Treated}) \\end{split} \\tag{4.3} \\end{equation}\\] 4.4.3 Comparing ATE and ATT ATE describes the treatment effect for the entire population, including individuals who would never end up getting the treatment in the real world. For the interest of testing theory or understanding general behavior, we care more about ATE. On the other hand, ATT describes the treatment effect on only the individuals who end up getting the treatment. For the interest of policy implementation, we care more about ATT. In well designed studies, ATE can be the same as ATT. 4.4.4 Counterfactual Model Now we can formally define the counterfactual model. We will then also illustrate the usage of the model with a hypothetical data set in R. Defining Treatment Assignment \\[D_i=D_1\\text{ if unit i received the treatment}\\] \\[D_i=D_0\\text{ if unit i did not receive treatment}\\] Defining Outcomes \\[Y_i:\\text{Observed outcome variable of interest for unit i}\\] \\[\\text{Note that the treatment occurs temporally before the outcomes}\\] Fundamental Problem of Causal Inference Recall that we are trying to answer this question. However, this leads us to a crucial obstacle. We can never observe the treated outcome and the control outcome for the same individual. Revisiting Delta and Average Treatment Effect In light of the aforementioned fundamental issue, we redefine the casual effect of the treatment (4.1) as the difference of its two potential outcomes. \\[\\delta \\text{ is }\\alpha_i=Y_{1i}-Y_{0i}\\] \\[Y_{di}=Y_{1i}=\\text{Potential outcome for unit i with treatment}\\] \\[Y_{di}=Y_{0i}=\\text{Potential outcome for unit i without treatment}\\] Based on the above, we will also redefine Average Treatment Effect (4.2) as follows: \\[\\alpha_{\\text{ATE}}=\\frac{\\Sigma Y_{1i}}{N}-\\frac{\\Sigma Y_{0i}}{N}=E[Y_{1i}-Y_{0i}]\\] Correspondingly, we redefine the Average Treatment Effect on the Treated (4.3) and also introduce the Average Treatment Effect on the Control (ATC) as follows: \\[\\alpha_{\\text{ATT}}=E[Y_{1i}-Y_{0i}|D_1]\\] \\[\\alpha_{\\text{ATC}}=E[Y_{1i}-Y_{0i}|D_0]\\] Illustrating Counterfactuals with R We will now use the unions.csv data set to illustrate the concept of counterfactuals and the Fundamental Problem of Causal Inference. Let’s view the structure of the available data. The variables in the data set are: surname: Individual’s Last Name union_member: Union Member Logical Variable (1=“Yes”; 0=“No”) ideology_y1: Ideology If a Union Member (Ranges from -10=“far-left” to 10=“far-right”) ideology_y0: Ideology If Not a Union Member (Ranges from -10=“far-left” to 10=“far-right”) ITE: Individual Treatment Effect (incomplete) Table 4.2: The Incomplete Content ofunions.csv surname union_member ideology_y1 ideology_y0 ITE Abernathy 0 4 7 NA Baca 1 -2 -2 NA Chang 0 3 2 NA Dunn 1 -4 1 NA Emami 1 -2 1 NA Fan 0 0 0 NA We can see that the rightmost column is still empty. The column is designated for the Individual Treatment Effect (ITE). In other words, it is actually \\(\\delta\\) or \\(\\alpha_{i}\\). Let’s calculate the treatment effect as follows, and assign the result to the column: unions$ITE &lt;- unions$ideology_y1 - unions$ideology_y0 Table 4.3: The Complete Content ofunions.csv surname union_member ideology_y1 ideology_y0 ITE Abernathy 0 4 7 -3 Baca 1 -2 -2 0 Chang 0 3 2 1 Dunn 1 -4 1 -5 Emami 1 -2 1 -3 Fan 0 0 0 0 Thus, we have updated our data set and we can now simply calculate the ATE as the mean of ITE: mean(unions$ITE) ## [1] -1 Additionally, we can check the ‘most affected’ individual by creating a vector containing the absolute values of the ITE, and then we look for the index containing the maximum value: # Create the vector ITE_abs &lt;- abs(unions$ITE) # Find the corresponding surname of the maximum absolute ITE unions$surname[which.max(ITE_abs)] ## [1] &quot;Dunn &quot; To calculate ATT, we can filter out the control group and then use the mean() function as follows: mean(unions$ITE[unions$union_member == 1]) ## [1] -2 Do you see the main issue? In real life, the table is incomplete! We are not able to directly calculate ITE, as an individual cannot be a union member and not a union member at the same time. In other words, in practice, our data would look something like this: Table 4.4: The Problem of Unobservables surname union_member ideology_y1 ideology_y0 ITE Abernathy 0 NA 7 NA Baca 1 -2 NA NA Chang 0 NA 2 NA Dunn 1 -4 NA NA Emami 1 -2 NA NA Fan 0 NA 0 NA Therefore, observed outcomes are realized as \\[\\begin{equation} Y_i=D_i \\times Y_{1i}+(1-D_i)\\times Y_{0i} \\tag{4.4} \\end{equation}\\] Additionally, our ‘best guess’ is the Difference-in-Means (DM) estimator, which is based on the outcomes that we actually see in the above table, defined as \\[E[Y_i|D_i=1]-E[Y_i|D_i=0]\\] We calculate this estimator in R with the following: is_union &lt;- (unions$union_member == 1) not_union &lt;- (unions$union_member == 0) mean(unions$ideology_y1[is_union]) - mean(unions$ideology_y0[not_union]) ## [1] -1.636364 As you can see, it is not equal to the ATE we calculated previously. How do we show this mathematically? Well we can use a simple mathematical trick. We start off with the Difference-in-Means estimator as follows: \\[E[Y_i|D_i=1]-E[Y_i|D_i=0]\\] Under the observed outcome framework (4.4) the above equation is equivalent to: \\[E[Y_{1i}|D_i=1]-E[Y_{0i}|D_i=0]\\] We then add and subtract \\(E[Y_{0i}|D_i=1]\\) to the above term. \\[E[Y_{1i}|D_i=1]-E[Y_{0i}|D_i=1]+E[Y_{0i}|D_i=1]-E[Y_{0i}|D_i=0]\\] This effectively does not change the equation. The term is an unobservable, as it is the potential outcome if unit \\(i\\) did not receive treatment, given that unit \\(i\\) is in the treatment group. Combining the first two terms, we have: \\[E[Y_{1i}-Y_{0i}|D_i=1]+E[Y_{0i}|D_i=1]-E[Y_{0i}|D_i=0]\\] What do these terms mean exactly? \\[E[Y_{1i}-Y_{0i}|D_i=1] \\text{ is the ATT}\\] \\[E[Y_{0i}|D_i=1]-E[Y_{0i}|D_i=0] \\text{ is the (selection) bias}\\] Therefore, in causal inference, our aim is to analyze ATT, whilst minimizing the bias term. 4.5 Framework and Assumptions Framework In RCT, we rely on an appropriate assignment mechanism to fill in the missing potential outcomes and get rid of the bias. In other words, the randomization stage deals with the ‘apples and oranges’ problem between the treatment and control. Assumptions Now we introduce the two necessary assumptions for Randomized Controlled Trials. Excludability The treatment is the sole causal agent and there are no confounding factors (factors that are associated with the treatment but not the intention of the treatment). Example of violation: Hawthorne effect (people behave differently when they are aware of being in an experiment). Non-interference/no spillovers The individual potential outcomes of each unit do not depend on the treatment status of any units other than themselves. Example of violation: The treated individuals transfer resources to the control individuals. 4.6 Caveats and Difficulties Random Assignment is Not Random Sampling These two processes are completely separate, and happen in different parts of the experiment. Internal and External Validity We should always ask ourselves if the effect of interest can be translated to the general population Difficulties in Design We need a simple design that works We need to know the subjects well to design an effective experiment Experimental design is more of an art than a science Difficulties in Implementation Non-compliance: Individuals do not do what you ask them to do Attrition: People quit during the experiment Spill-overs: Individuals in the control group interact with treated individuals or are indirectly affected by them through the changed social and economic conditions caused by the treatment Ethics: Ethical issues can be a serious concern 4.7 Case Studies 4.7.1 Labor Market Experiment This example is based on a well-known paper on racial bias in the labor market: “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination” by Marianne Bertrand and Sendhil Mullainathan (thus BM for short). The paper used a Randomized Controlled Trial experiment. The paper wanted to see if there was racial discrimination, particularly when comparing White and Black job seekers, in the Chicago and Boston labor market. The researchers created fake CVs with White and Black sounding names, and then diversified the CV quality and postal codes. They found that people who had Black sounding names had a lower callback rate. Moreover, it seemed that the quality of the resumes did not matter, as the better quality White resumes had 30% more callbacks than the worse quality ones, but the effect was not as great for African-American resumes. Additionally, better neighborhoods based on the assigned postal codes increased callback rates for White resumes but not for Black ones. The paper concludes that African-Americans face differential treatment when searching for jobs, and they find it hard to overcome this hurdle by improving their observable skills and credentials. In fact if African-Americans knew the economic returns for their skills, they rationally would be less willing to participate in skills enhancement programs. Depending on the available data, we can use different causal inference strategies or methodologies to our advantage. Here, let’s try and mimic the data exploration process that eventually lead to them using an RCT approach. Let’s view the structure of our data. We have the following variables: firstname: Individual’s First Name sex: Individual’s Biological Sex race: Individual’s Race call: Callback Logical Variable (1=“Yes; 0=”No”) # Generate the frequency table knitr::kable(head(resume), align = &quot;c&quot;, booktabs = TRUE, caption = &quot;The Structure of `resume.csv`&quot;) Table 4.5: The Structure of resume.csv firstname sex race call Allison female white 0 Kristen female white 0 Lakisha female black 0 Latonya female black 0 Carrie female white 0 Jay male white 0 First, we can tabulate the relationship between race and callback by creating a frequency table: # This stage just to format things better, so that the table is more readable resume_copy &lt;- resume resume_copy$race[resume_copy$race == &quot;white&quot;] &lt;- &quot;White&quot; resume_copy$race[resume_copy$race == &quot;black&quot;] &lt;- &quot;Black&quot; resume_copy$call[resume_copy$call == &quot;1&quot;] &lt;- &quot;Received Callback&quot; resume_copy$call[resume_copy$call == &quot;0&quot;] &lt;- &quot;Received No Callback&quot; # Then we show the table knitr::kable(table(resume_copy$race, resume_copy$call), align = &quot;c&quot;, booktabs = TRUE) Received Callback Received No Callback Black 157 2278 White 235 2200 Another way show a better picture of the relationship is to calculate the callback rates of the races: # Callback rate for &#39;White&#39; call_rate_w &lt;- mean(resume$call[resume$race == &quot;white&quot;]) # Callback rate for &#39;Black&#39; call_rate_b &lt;- mean(resume$call[resume$race == &quot;black&quot;]) Then, we can visualize the data and compare the two callback rates as with bar plots. # library(ggplot2) # library(ggthemes) summary_bw &lt;- data.frame(Callback_Rate = c(call_rate_b, call_rate_w), Race = c(&quot;Black&quot;, &quot;White&quot;)) # Here we create the bar plot ggplot(summary_bw, aes(x = Race, y = Callback_Rate, fill = Race)) + geom_bar(stat = &#39;identity&#39;, show.legend = FALSE) + xlab(&quot;Race&quot;) + ylab(&quot;Callback Rate&quot;) + scale_fill_manual(values = c(&quot;cyan&quot;, &quot;lightblue&quot;)) + theme_clean() We can also create new conditional columns, or dummy variables based on the race and sex variables. resume$BlackFemale &lt;- ifelse(resume$race == &quot;Black&quot; &amp; resume$sex == &quot;female&quot;, 1, 0) resume$BlackMale &lt;- ifelse(resume$race == &quot;Black&quot; &amp; resume$sex == &quot;male&quot;, 1, 0) resume$WhiteFemale &lt;- ifelse(resume$race == &quot;White&quot; &amp; resume$sex == &quot;female&quot;, 1, 0) resume$WhiteMale &lt;-ifelse(resume$race == &quot;White&quot; &amp; resume$sex == &quot;male&quot;, 1, 0) # See the updated data set knitr::kable(head(resume), booktabs = TRUE, align = &quot;c&quot;, caption = &quot;Adding New Dummy Variables to `resume.csv`&quot;) Table 4.6: Adding New Dummy Variables to resume.csv firstname sex race call BlackFemale BlackMale WhiteFemale WhiteMale Allison female white 0 0 0 0 0 Kristen female white 0 0 0 0 0 Lakisha female black 0 0 0 0 0 Latonya female black 0 0 0 0 0 Carrie female white 0 0 0 0 0 Jay male white 0 0 0 0 0 4.7.2 Corruption in Indonesia For the next example, we will use data from Benjamin A. Olken. 2007. “Monitoring Corruption: Evidence from a Field Experiment in Indonesia.” Journal of Political Economy, 115: 300-249. The objective of this experiment is to evaluate two interventions aimed at reducing corruption in road building projects in Indonesian villages. One treatment is audits by engineers; the other is encouraging community participation in monitoring. This problem focuses on the latter intervention, which consists of inviting villagers to public meetings where project officials account for budget expenditures. In this paper, they used stratified randomization as the assignment procedure. Here are the variables that are included in the data set: The variables in the data set are: pct_missing: Percent Expenditures Missing treat_invite: Treatment Assignment (1=“Yes; 0=”No”) head_edu: Village Head Education mosques: Mosques per 1,000 pct_poor: Percent of Households Below the Poverty Line total_budget: Total Budget (Rp. Million) (Determined Prior to Intervention) The main dependent variable is pct_missing: a measure of the difference between what the villages claim they spend on road construction and an independent estimate of what the villages actually spend. Here, we will actually run the regression and conduct a t-test, but we will conduct the data exploration process as we did in the previous case study. Let’s compare the number of treated and untreated: suppressMessages(library(stargazer)) # Generate the frequency table knitr::kable(table(data.frame(&quot;Treatment&quot; = olken$treat_invite)), align = &quot;c&quot;, booktabs = TRUE) Treatment Freq 0 191 1 376 Despite the large difference in frequencies, this is does not harm our experiment. We can still conduct a t-test later on to check the statistical significance our findings. As the data set has already undergone assignment process, with details discussed in the paper, we can now compute the Difference-in-Means (DM) to estimate ATE as follows: # Define the logical conditions is_treatment &lt;- (olken$treat_invite == 1) is_control &lt;- (olken$treat_invite == 0) # Calculate DM mean(olken$pct_missing[is_treatment], na.rm = TRUE) - mean(olken$pct_missing[is_control], na.rm = TRUE) ## [1] -0.02314737 Alternatively, we can also estimate the ATE with linear regression as follows: # Linear Regression olken_ate_estimate &lt;- lm(pct_missing ~ treat_invite, data = olken) # Display the coefficients in a stargazer table # Display 8 digits to match the DM estimate stargazer(coef(summary(olken_ate_estimate)), type = &quot;html&quot;, flip = TRUE, digits = 8) (Intercept) treat_invite Estimate 0.25210560 -0.02314737 Std. Error 0.02699349 0.03321720 t value 9.33949600 -0.69684900 Pr(&gt; | t| ) 0 0.48623820 We can see that the estimate is indeed the same with the Difference-in-Means (DM) estimator we previously calculated, that is -0.02314737. Additionally, we can try and rerun the regression with added covariables. # Multiple Linear Regression olken_covariables &lt;- lm(pct_missing ~ ., olken) # Display the coefficients in a stargazer table stargazer(coef(summary(olken_covariables)), type = &quot;html&quot;, flip = TRUE) (Intercept) treat_invite head_edu mosques pct_poor total_budget Estimate 0.390 -0.026 -0.006 -0.048 -0.118 0.001 Std. Error 0.087 0.033 0.006 0.019 0.075 0.0003 t value 4.491 -0.797 -0.949 -2.540 -1.574 1.846 Pr(&gt; | t| ) 0.00001 0.426 0.343 0.011 0.116 0.066 We can see that the estimate did indeed change by approximately 0.003, as it is now -0.026. As we add covariables, we must check the covariate balance in this data set. For example, here we check whether the education of the head of the household is roughly the same between the treatment and control groups with a t-test. # Conduct a t-test for head of household education t_head_edu &lt;- t.test(olken$head_edu[is_treatment], olken$head_edu[is_control]) Table 4.7: Estimation with t-test, and the 95% CI Estimate t value Pr(&gt;|t|) CI Lower CI Upper -0.0686555 -0.2849279 0.7758529 -0.5424163 0.4051052 In this illustration, we will use a t-test, but we show that you can run a linear regression model with the covariable as the dependent variable (e.g head_edu), and the treatment assignment as the independent variable. The results will be the same. Table 4.8: Linear Model Estimation Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.5026178 0.1958546 58.7303973 0.0000000 treat_invite -0.0686555 0.2410543 -0.2848135 0.7758923 We may see minor differences of the 95% CI with the t-test caused by the definition of the functions in R. These are not to be considered, and the values are still practically identical. Table 4.9: Linear Model 95% CI CI Lower CI Upper (Intercept) 11.1179184 11.8873172 treat_invite -0.5421367 0.4048256 Next, we will repeat the same process as above for the other covariables: # Conducts a t-test for mosques t_mosques &lt;- t.test(olken$mosques[is_treatment], olken$mosques[is_control]) # Conducts a t-test for pct_poor t_pct_poor &lt;- t.test(olken$pct_poor[is_treatment], olken$pct_poor[is_control]) # Conducts a t-test for total_budget t_total_budget &lt;- t.test(olken$total_budget[is_treatment], olken$total_budget[is_control]) We can tabulate all the results as follows: # The following code creates the table results &lt;- matrix(nrow = 4, ncol = 4, NA) rownames(results) &lt;- c(&quot;Control&quot;, # Control Group Mean &quot;Treatment&quot;, # Treatment Group Mean &quot;Difference&quot;, # Difference in Means Between Treatment and Control &quot;P-value&quot;) # Associated P-value colnames(results) &lt;- c(&quot;head_edu&quot;, &quot;mosques&quot;, &quot;pct_poor&quot;, &quot;total_budget&quot;) # Let&#39;s enter our data results[, 1] &lt;- c(t_head_edu$estimate[1], t_head_edu$estimate[2], t_head_edu$estimate[2] - t_head_edu$estimate[1], t_head_edu$p.value) results[, 2] &lt;- c(t_mosques$estimate[1], t_mosques$estimate[2], t_mosques$estimate[2] - t_mosques$estimate[1], t_mosques$p.value) results[, 3] &lt;- c(t_pct_poor$estimate[1], t_pct_poor$estimate[2], t_pct_poor$estimate[2] - t_pct_poor$estimate[1], t_pct_poor$p.value) results[, 4] &lt;- c(t_total_budget$estimate[1], t_total_budget$estimate[2], t_total_budget$estimate[2] - t_total_budget$estimate[1], t_total_budget$p.value) # Print out the table stargazer(results, type = &quot;html&quot;) head_edu mosques pct_poor total_budget Control 11.434 1.412 0.414 80.223 Treatment 11.503 1.474 0.405 81.983 Difference 0.069 0.062 -0.009 1.760 P-value 0.776 0.405 0.640 0.674 Looking at the t-tests, all the p-values are greater than 0.1. We fail to reject the null hypothesis that the true difference in means is equal to zero and thus it seems that the two groups are balanced. We show that we are comparing apples with apples. "],["iv.html", "5 Instrumental Variables 5.1 Framework and Assumptions 5.2 Section 2: Applied Problems", " 5 Instrumental Variables 5.1 Framework and Assumptions Assumptions Here, we discuss four assumptions that are necessary for a valid IV. Independence. ‘z’ is as good as randomly assigned. Exclusion Restriction ‘z’ does not affect ‘Y’ directly, or in another words, once we know ‘D’, we don’t need to know ‘z’. Relevance ‘z’ has some power to influence the treatment. Monotonicity subjects receiving the treatment assignment (‘z’ = 1) are more likely to actually get treated (‘D’ = 1), and there are no cases of ‘defiers’, that is people whom we don’t want to treat who actually get treated. 5.1.1 Question 2 Now, with these assumptions in mind, we can derive the two-stage least squares (2SLS) estimator in the mincer regression of return to schooling. In the mis-specified Mincer regression, we have: \\[\\begin{align} Y_{i} &amp; = \\alpha + \\rho S_{i} + \\eta_{i} \\\\ \\eta_{i} &amp; = \\gamma A_{i} + \\nu_{i} \\end{align}\\] Suppose an IV ‘Z’ that is correlated with ‘S’, let’s estimate the model with ‘Z’. \\[\\begin{align} Cov(Y, Z) &amp; = Cov(\\alpha + \\rho S + \\gamma A + \\nu, Z) \\\\ &amp; = \\rho Cov(S, Z) + \\gamma Cov(A, Z) + Cov(\\nu, Z) \\end{align}\\] Now, suppose the two following conditions hold: \\[\\begin{align} &amp; 1.) \\, Cov(S, Z) \\neq 0 - \\text{&quot;first stage&quot; exists. &#39;S&#39; and &#39;Z&#39; are correlated.} \\\\ &amp; 2.) \\, Cov(A, Z) = Cov(\\nu, Z) = 0 - \\text{&quot;exclusion restriction&quot;.} \\\\ &amp; \\text{&#39;Z&#39; is orthogonal to the factors in } \\eta \\text{, such as unobserved ability or structural disturbance term, } \\eta . \\end{align}\\] \\[\\begin{align} \\rho_{IV} = \\frac{Cov(Y, Z)}{Cov(S, Z)} = \\rho \\end{align}\\] Summing up the above procedures, we have a structural model. \\[\\begin{align} Y_{i} &amp; = \\alpha + \\rho S_{i} + \\eta_{i} \\\\ \\eta_{i} &amp; = \\gamma A_{i} + \\nu_{i} \\end{align}\\] We have the first stage regression. \\[\\begin{align} S_{i} = \\alpha + \\rho Z_{i} + \\zeta_{i} \\end{align}\\] \\[\\begin{align} \\text{By which we calculate the fitted value } \\widehat{S} \\end{align}\\] And we have the second stage regression. \\[\\begin{align} Y_{i} = \\alpha + \\rho S_{i} + \\nu_{i} \\end{align}\\] Finally, we can calculate the Wald estimator, which is exactly the two-stage least square (2SLS) estimator. \\[\\begin{align} \\rho_{IV} = \\frac{Cov(Y, Z)}{Cov(S, Z)} = \\rho \\end{align}\\] 5.1.2 Question 3 Let’s examine the validity of bad weather as an IV in studying the effect of the scale of street protests on policy changes. We assess the four assumptions that are necessary for a valid IV. a.) Independence: bad weather is somewhat randomly assigned, however nowadays with good weather forecasts people can have numerical expectations on weather, thus the covariance of bad weather with protests may not be zero. Therefore independence is questionable. b.) Exclusion Restriction: bad weather does not affect policy directly. Therefore, this assumption is fulfilled. c.) Relevance: bad weather has some power to influence the treatment, that is the protests. Thus, this assumption is fulfilled. d.) Monotonicity: subjects receiving bad weather assignment (‘z’ = 1) are less likely to actually do protests (‘D’ = 1), and there could be ‘defiers’. The problem is that nowadays any type of policy can receive ‘soft form protests’ through social media, and if taken to the extreme, public officials can receive backlash in certain cases. Thus, there will be no ‘pure’ instance of policy without treatment. 5.2 Section 2: Applied Problems 5.2.1 Question 1 (IV Regression with Simulated Data) In this exercise, we will generate some data and then use them to show how IVs work in regression. Let’s first load the package “MASS” which has a multivariate normal distribution function mvrnorm(). suppressMessages(library(MASS)) We also load the package “AER” which will be used to conduct IV regression. suppressMessages(library(AER)) Additionally, we will load the package “stargazer” to create regression tables. suppressMessages(library(stargazer)) Finally, we have to load all the data set for this problem set. setwd(&quot;~/R_Programming/ECON3284/ProblemSet3&quot;) ajr &lt;- read.csv(file.path(getwd(), &quot;ajr.csv&quot;)) 5.2.1.1 Part A (Simulating Data for IV Regression) We simulate data for two-stage least squares estimation using the following procedure: i.) We set the seed of the random number generator to 1234. set.seed(1234) ii.) We first define a variable called ‘n’ to use as our sample size. n &lt;- 1000 iii.) We create a matrix called Rho that will serve as the variance covariance matrix of the error terms ‘e’ and ‘v’ in the simulation. Set the variance of each to ‘1’ and the correlation between them to ‘0.5’. Rho &lt;- matrix(c(1, 0.5, 0.5, 1), 2, 2, byrow = TRUE) iv.) We make ‘n’ bivariate normal draws with mean zero and variance-covariance matrix ‘Rho’. We store the result as ‘sims’. sims &lt;- mvrnorm(n, c(0, 0), Rho) v.) We extract the first column of ‘sims’ and store it as a vector called ‘e’. Then, we extract the second column and store it as a vector called ‘v’. e &lt;- sims[,1] ; v &lt;- sims[,2] vi.) We make ‘n’ Uniform(0,1) random draws and store the result in a vector called ‘z’. z &lt;- runif(n) vii.) We generate a vector called ‘x’ using the IV first-stage equation with pi zero = 0.5 and pi one = 0.08. x &lt;- 0.5 + 0.8 * z + v viii.) We generate a vector called ‘y’ using the IV structural equation with beta zero = -0.3 and beta one = 1. y &lt;- -0.3 + x + e 5.2.1.2 Part B (Ordinary Least Squares) At this stage, we run an OLS regression of ‘y’ on ‘x’. ols_results &lt;- lm(y ~ x) stargazer(ols_results, type = &#39;html&#39;, omit.stat = c(&#39;f&#39;, &#39;ser&#39;), header = FALSE) Dependent variable: y x 1.461*** (0.025) Constant -0.740*** (0.035) Observations 1,000 R2 0.767 Adjusted R2 0.767 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 These results do not give the causal effect of ‘x’ on ‘y’ because when we first defined ‘x’ and ‘y’, we used the error terms ‘e’ and ‘v’. These two error terms, by design, were correlated with each other, and that means we may overestimate or underestimate the correlation between ‘x’ and ‘y’. 5.2.1.3 Part C (Two Stage Least Squares by Hand) i.) Here we first run an OLS regression of ‘x’ on ‘z’ store the result as ‘first_stage’. first_stage_q1 &lt;- lm(x ~ z) stargazer(first_stage_q1, type = &#39;html&#39;, omit.stat = c(&#39;f&#39;, &#39;ser&#39;), header = FALSE) Dependent variable: x z 0.819*** (0.110) Constant 0.475*** (0.064) Observations 1,000 R2 0.053 Adjusted R2 0.052 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 ii.) Then, we run an OLS regression of ‘y’ on ‘z’ and store the result as ‘reduced_form’. reduced_form_q1 &lt;- lm(y ~ z) stargazer(reduced_form_q1, type = &#39;html&#39;, omit.stat = c(&#39;f&#39;, &#39;ser&#39;), header = FALSE) Dependent variable: y z 0.785*** (0.187) Constant 0.162 (0.109) Observations 1,000 R2 0.017 Adjusted R2 0.016 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 iii.) Now, we divide the slope from ‘reduced_form’ by the slope from ‘first_stage’. summary(reduced_form_q1)$coefficients[2] / summary(first_stage_q1)$coefficients[2] ## [1] 0.9582441 iv.) What we have done in the previous steps is compute the first stage of a 2SLS model, along with a reduced form of the model. We then extract the two betas, and dividing the reduced form by the first stage estimator, we calculate the Wald estimator which is actually precisely the so-called 2SLS estimator. 5.2.1.4 Part D (IV Regression) i.) Here, we use ‘ivreg’ to carry out IV regression that we did by hand in the preceding exercise and then store the results as ‘iv_results’. iv_results &lt;- ivreg(y ~ x | z) ii.) Then we display the IV results using ‘stargazer’. stargazer(iv_results, type = &#39;html&#39;, omit.stat = c(&#39;f&#39;, &#39;ser&#39;), header = FALSE) Dependent variable: y x 0.958*** (0.131) Constant -0.293** (0.121) Observations 1,000 R2 0.676 Adjusted R2 0.676 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 iii.) Additionally, we calculate an approximate 95% confidence interval for the causal effect of ‘x’ on ‘y’ using ‘iv_results’. confint(iv_results, parm = &#39;x&#39;, level = 0.95) ## 2.5 % 97.5 % ## x 0.701438 1.21505 iv.) If we view the results in part C and D, we will find that we obtain the same causality from the manual 2SLS approach and the direct ‘ivreg’ function. 5.2.2 Question 2 (Colonial Origins of Comparative Development, AJR) In this exercise, we’ll work with a data set from the paper ‘The Colonial Origins of Comparative Development’ by Acemoglu, Johnson, and Robinson. I’ll refer the paper as AJR in the rest of this document. 5.2.2.1 Part A (Background) i.) AJR tries to answer the effect of institutions on income per capita. ii.) The core of AJR’s theory is based on several things. Firstly, they separate countries based on types of European colonization styles that they encountered in the past. They argued that places with higher feasibility of settlement allowed historical Europeans to construct proper European-style institutions with a focus on private property and checks against government power. Countries that fall into this category include Australia, New Zealand, Canada, and the United States. However, if the colonizers viewed a region to be difficult, then they would set up ‘extraction states’ that served a purpose of sending back resources to the colonialists’ mainland. Therefore, summarizing the logic, it can be depicted as follows: (potential) settler mortality –&gt; settlements –&gt; early institutions –&gt; current institutions –&gt; current –&gt; performance. iii.) For ‘z’ to be a valid instrument, it must satisfy two key assumptions. First it must be relevant: correlated with ‘x’. Second, it must be excluded: it must only be related to y through its effect on ‘x’. In the context of AJR, these assumptions mean that settler mortality must be correlated with the availability of institutions, and it must only be related with GDP per capita through its effects on the availability of institutions. 5.2.2.2 Part B (OLS Regression) i.) First we regress ‘loggdp’ on ‘risk’ and store the result in an object called “ols”. ols &lt;- lm(loggdp ~ risk, data = ajr) ii.) Then we display the model summary. stargazer(ols, type = &#39;html&#39;, omit.stat = c(&#39;f&#39;, &#39;ser&#39;), header = FALSE) Dependent variable: loggdp risk 0.505*** (0.062) Constant 4.731*** (0.415) Observations 62 R2 0.523 Adjusted R2 0.515 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 iii.) We can’t interpret the results of ‘ols’ causally because we know the possibility of better institutions resulting in better developed countries; however, countries with high GDP or income can afford better institutions. Therefore, there is a simultaneous causality bias i.e ‘x’ causes ‘y’ and ‘y’ causes ‘x’. Additionally, there are many omitted determinants of income differences that will naturally be correlated with institutions. Finally, the measures of institutions are constructed ex post, and the analysts may have had a natural bias in seeing better institutions in richer places. As well as these problems introducing positive bias in the OLS estimates, the fact that the institutions variable is measured with considerable error and corresponds poorly to the “cluster of institutions” that matter in practice creates attenuation and may bias the OLS estimates downwards. 5.2.2.3 Part C (IV Regression) i.) Now, we estimate the first-stage regression of ‘risk’ on ‘logmort0’ and store our results in an object called “first_stage”. first_stage &lt;- lm(risk ~ logmort0, data = ajr) Then we display this model’s summary. stargazer(first_stage, type = &#39;html&#39;, omit.stat = c(&#39;f&#39;, &#39;ser&#39;), header = FALSE) Dependent variable: risk logmort0 -0.620*** (0.131) Constant 9.388*** (0.633) Observations 62 R2 0.272 Adjusted R2 0.260 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 ii.) We estimate the reduced-form regression of ‘loggdp’ on ‘logmort0’ and we store our results in an object called ‘reduced_form’. reduced_form &lt;- lm(loggdp ~ logmort0, data = ajr) Then we display this model’s summary. stargazer(reduced_form, type = &#39;html&#39;, omit.stat = c(&#39;f&#39;, &#39;ser&#39;), header = FALSE) Dependent variable: loggdp logmort0 -0.561*** (0.079) Constant 10.634*** (0.382) Observations 62 R2 0.457 Adjusted R2 0.448 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 iii.) Additionally, we use the ‘ivreg’ function from ‘AER’ to carry out an IV regression of ‘loggdp’ on ‘risk’ using ‘logmort0’ as an instrument for ‘risk’ and then store our results in an object called ‘iv’. iv &lt;- ivreg(loggdp ~ risk | logmort0, data = ajr) We also display this model’s summary. stargazer(iv, type = &#39;html&#39;, omit.stat = c(&#39;f&#39;, &#39;ser&#39;), header = FALSE) Dependent variable: loggdp risk 0.905*** (0.155) Constant 2.135** (1.014) Observations 62 R2 0.195 Adjusted R2 0.181 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 iv.) Looking at the summary of ‘iv’, the results are actually larger compared to ‘ols’, i.e ‘risk’ has higher causality. v.) If we use the manual 2SLS approach with ‘first_stage’ and ‘reduced_form’, we will also get the same result. summary(reduced_form)$coefficients[2] / summary(first_stage)$coefficients[2] ## [1] 0.9052929 5.2.2.4 Part D (Critique) Let’s consider a potential criticism of AJR. The critique depends on two claims. Claim #1: a country’s current disease environment, e.g. the prevalence of malaria, is an important determinant of GDP/capita. Claim #2: a country’s disease environment today depends on its disease environment in the past, which in turn affected early European settler mortality. i.) Both claims taken together would call into question the IV results from Part C above as initially we would combine ‘malaria’ into ‘u’, i.e the noise factor. This is problematic because that means a part of ‘u’ is actually correlated with our IV, making it invalid. Also, it means that a part of ‘u’ actually causes higher GDP, and therefore we are missing an important component. ii.) We should consider re-running our IV analysis from Part C including malaria as an additional regressor because hopefully, as we separate malaria from ‘u’, we can get an IV that is not correlated with ‘u’. iii.) Here, we repeat Part B including malaria as an additional regressor. updated_ols &lt;- lm(loggdp ~ risk + malaria, data = ajr) We display the results in the following table. stargazer(updated_ols, type = &#39;html&#39;, omit.stat = c(&#39;f&#39;, &#39;ser&#39;), header = FALSE) Dependent variable: loggdp risk 0.339*** (0.055) malaria -1.145*** (0.183) Constant 6.296*** (0.409) Observations 62 R2 0.714 Adjusted R2 0.704 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 iv.) Then, we repeat the first section of Part C, adding malaria to the first-stage regression. updated_first_stage &lt;- lm(risk + malaria ~ logmort0 + malaria, data = ajr) We display the results in the following table. stargazer(updated_first_stage, type = &#39;html&#39;, omit.stat = c(&#39;f&#39;, &#39;ser&#39;), header = FALSE) Dependent variable: risk + malaria logmort0 -0.438** (0.188) malaria 0.304 (0.522) Constant 8.834*** (0.754) Observations 62 R2 0.119 Adjusted R2 0.089 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 v.) Additionally, we also repeat the third section of Part C, including malaria in the IV regression. We treat malaria as exogenous. updated_iv &lt;- ivreg(loggdp ~ risk + malaria | logmort0 + malaria, data = ajr) We display the results in the following table. stargazer(updated_iv, type = &#39;html&#39;, omit.stat = c(&#39;f&#39;, &#39;ser&#39;), header = FALSE) Dependent variable: loggdp risk 0.589** (0.222) malaria -0.751* (0.396) Constant 4.505*** (1.591) Observations 62 R2 0.615 Adjusted R2 0.602 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 vi.) The criticism of AJR based on a country’s disease environment is valid, because here we can clearly see that institutions as a causal factor is comparably weaker in this model as we add malaria as an additional regressor. "],["rdd.html", "6 Regression Discontinuity Design", " 6 Regression Discontinuity Design "],["did.html", "7 Difference-in-Differences", " 7 Difference-in-Differences "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
